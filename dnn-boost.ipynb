{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Somatic Mutation Identification with DNN-Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will demonstrate the somatic mutation identification of variants dataset from tumor-normal paired pancreatic cancer data. We will first use feature selection on the data using XGBoost and then train a deep neural netwook on the data with the selected features according to XGBoost (using Tensorflow).\n",
    "\n",
    "Note: Before running this tutorial, please install the scipy, pandas, matplotlib, Tensorflow, Sklearn, imblearn, and xgboost packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, average_precision_score, precision_recall_curve, plot_precision_recall_curve, plot_roc_curve\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset of variants from tumor-normal pancreatic cancer WES data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1fdcd44a3399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download the dataset from the training-data folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpancreatic_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pancreatic_cancer_tumor-normal.csv'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Download the dataset from the training-data folder\n",
    "pancreatic_data = pd.read_csv('Pancreatic_cancer_tumor-normal.csv',  header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Column Target matching Dictionary value\n",
    "map_labelMut = {'somatic_mutation': 1,\n",
    "                'germline': 0}\n",
    "\n",
    "pancreatic_data['target'] = pancreatic_data['label_mut'].map(map_labelMut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = pancreatic_data[['ExAC_ALL','SIFT_score','Polyphen2_HDIV_score','Polyphen2_HVAR_score',\n",
    "                          'LRT_score','MutationTaster_score','MutationAssessor_score','FATHMM_score',\n",
    "                          'PROVEAN_score','VEST3_score','MetaSVM_score','MetaLR_score','M-CAP_score',\n",
    "                          'CADD_raw','CADD_phred','DANN_score','fathmm-MKL_coding_score','Eigen-raw',\n",
    "                          'Eigen-PC-raw','GenoCanyon_score','integrated_fitCons_score','GERP++_RS',\n",
    "                          'phyloP100way_vertebrate','phyloP20way_mammalian','phastCons100way_vertebrate',\n",
    "                          'phastCons20way_mammalian','SiPhy_29way_logOdds','gnomAD_exome_ALL','FS',\n",
    "                          'MQRankSum','QD','ReadPosRankSum','RPB','VAF','MQ','target']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = X_copy.columns[-1]\n",
    "X = X_copy.drop(label, axis = 1)\n",
    "Y = X_copy.drop(X, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Dataset Handling\n",
    "Because we have imbalanced datasets for the two classes, where the samples for \"Confirmed Somatic Variant\" dataset is much higher compare to the \"Germline\" class, we will create synthetic samples for the minority (\"Germline\") class. We used SMOTEENN for over-sampling (SMOTE) and noise cleaning (ENN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters for SMOTE, ENN, and SMOTE-ENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a SMOTE for oversampling\n",
    "smote = SMOTE(sampling_strategy=0.2, k_neighbors=7)\n",
    "\n",
    "# Create a ENN for undersampling\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority', n_neighbors=10)\n",
    "\n",
    "#Define the SMOTEENN resampling\n",
    "smote_enn = SMOTEENN(enn=enn, smote=smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_enn = smote_enn.fit(x_train, y_train)\n",
    "X_train_rsam, Y_train_rsam = smote_enn.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ohe_train= pd.get_dummies(Y_train_rsam).astype(float).values\n",
    "Y_ohe_train= Y_train_rsam.values.ravel()\n",
    "X_ohe_train = X_train_rsam.astype(float).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_enn = smote_enn.fit(x_test, y_test)\n",
    "X_test_rsam, Y_test_rsam = smote_enn.fit_resample(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ohe_test = pd.get_dummies(Y_test_rsam).astype(float).values\n",
    "Y_ohe_test = Y_test_rsam.values.ravel()\n",
    "X_ohe_test = X_test_rsam.astype(float).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into DMatrices\n",
    "In order to use the native API for XGBoost, we will first need to build DMatrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train_rsam, label=Y_ohe_train)\n",
    "dtest = xgb.DMatrix(X_test_rsam, label=Y_ohe_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use mean absolute error (MAE) to evaluate the quality of our data train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth':3,\n",
    "    'min_child_weight': 1,\n",
    "    'eta': 0.1,\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 0.1,\n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric': 'mae'\n",
    "}\n",
    "\n",
    "num_boost_round = 999\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = model.best_iteration + 1\n",
    "best_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the features based on the feature importance score, in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.gcf()\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "xgb.plot_importance(loaded_model, max_num_features=100, height=0.5, ax=ax, importance_type='gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the feature importance score according to 'gain' value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_important = best_model.get_score(importance_type='gain')\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe data_xgb below is consisted of the features ranked according to its feature importance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "data_xgb = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "print(data_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the 24 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_24=data_xgb[:24]\n",
    "print(feature_24.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Somatic Mutation Classification with DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the 24 features from the pancreatic tumor-normal data as the input for the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_24=pancreatic_data[['ExAC_ALL', 'FS', 'MetaLR_score', 'DANN_score', 'gnomAD_exome_ALL',\n",
    "       'MutationTaster_score', 'GenoCanyon_score',\n",
    "       'phastCons100way_vertebrate', 'MetaSVM_score', 'SIFT_score',\n",
    "       'phyloP20way_mammalian', 'SiPhy_29way_logOdds', 'VEST3_score',\n",
    "       'CADD_raw', 'Eigen-raw', 'M-CAP_score', 'MutationAssessor_score',\n",
    "       'phyloP100way_vertebrate', 'QD', 'PROVEAN_score', 'RPB', 'MQ',\n",
    "       'LRT_score', 'CADD_phred', 'label_mut']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dataset with the full features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = pancreatic_data.columns[-1]\n",
    "#X = pancreatic_data.drop(label, axis = 1)\n",
    "#Y = pancreatic_data.drop(X, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dataset with the 24 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = feature_24.columns[-1]\n",
    "X = feature_24.drop(label, axis = 1)\n",
    "Y = feature_24.drop(X, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('One-hot encoding labels.')\n",
    "Y_ohe= pd.get_dummies(Y).astype(float).values\n",
    "print(Y_ohe)\n",
    "# Get training data as numpy array\n",
    "# because of non overlap\n",
    "X_ohe = X.astype(float).values\n",
    "print('Training shape is: ', X_ohe.shape)\n",
    "print('Labels encoded shape is: ', Y_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the train and test data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_ohe, Y_ohe, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the SMOTE-ENN to the training data of DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_enn = smote_enn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the F1-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "batch_size=100\n",
    "loss_function = categorical_crossentropy\n",
    "num_classes = 2\n",
    "no_epochs=100\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "verbosity = 1\n",
    "num_folds = 10\n",
    "input_shape = x_train.shape[1]\n",
    "\n",
    "# Define the performance metrics for the model\n",
    "METRICS = ['accuracy',\n",
    "           tf.keras.metrics.AUC(name='auc'),\n",
    "           tf.keras.metrics.Precision(name='precision'),\n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "           tf.keras.metrics.TruePositives(name='tp'),\n",
    "           tf.keras.metrics.FalsePositives(name='fp'),\n",
    "           tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "           tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "           f1_metric\n",
    "          ]\n",
    "\n",
    "# Create Model\n",
    "def make_model(metrics=METRICS, output_bias=None):\n",
    "    model = Sequential([\n",
    "        layers.Dense(64, input_dim=input_shape, activation='relu', kernel_initializer='normal', kernel_regularizer=l2(0.01), kernel_constraint=MaxNorm(5)),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        layers.Dense(num_classes, kernel_initializer='normal', activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        metrics=METRICS)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define per-fold training score containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_training_fold = []\n",
    "fp_training_fold = []\n",
    "tn_training_fold = []\n",
    "fn_training_fold = []\n",
    "loss_training_fold = []\n",
    "auc_training_fold = []\n",
    "acc_training_fold = []\n",
    "precision_training_fold = []\n",
    "recall_training_fold = []\n",
    "training_f1_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define per-fold validation score containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_per_fold = []\n",
    "fp_per_fold = []\n",
    "tn_per_fold = []\n",
    "fn_per_fold = []\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "auc_per_fold = []\n",
    "precision_per_fold = []\n",
    "prc_per_fold = []\n",
    "recall_per_fold = []\n",
    "val_f1scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "start = time.time()\n",
    "for train, test in kfold.split(x_train, y_train):\n",
    "    \n",
    "    X_train_rsam, Y_train_rsam = smote_enn.fit_resample(x_train[train], y_train[train])\n",
    "    Y_train_rsam = np.eye(2)[Y_train_rsam.flatten()]\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    # Fit data to model\n",
    "    name_weights = \"final_model_fold\" + str(fold_no) + \"_weights.h5\"\n",
    "    callbacks = get_callbacks(name_weights = name_weights, patience_lr=10)\n",
    "    history = model.fit(X_train_rsam, Y_train_rsam,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=no_epochs,\n",
    "                        verbose=verbosity,\n",
    "                        validation_data = (x_train[test], y_train[test]),\n",
    "                        callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    \n",
    "    loss_training_fold.append(history.history['loss'])\n",
    "    auc_training_fold.append(history.history['auc'] * 100)\n",
    "    tp_training_fold.append(history.history['tp'])\n",
    "    fp_training_fold.append(history.history['fp'])\n",
    "    tn_training_fold.append(history.history['tn'])\n",
    "    fn_training_fold.append(history.history['fn'])\n",
    "    acc_training_fold.append(history.history['accuracy'] * 100)\n",
    "    precision_training_fold.append(history.history['precision'] * 100)\n",
    "    recall_training_fold.append(history.history['recall'] * 100)\n",
    "    training_f1_scores.append(history.history['f1_metric'] * 100 )\n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(x_train[test], y_train[test], verbose=0)\n",
    "    print(f'Validation score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%; {model.metrics_names[2]} of {scores[2]*100}%; {model.metrics_names[3]} of {scores[3] * 100}%; {model.metrics_names[4]} of {scores[4] * 100}%; {model.metrics_names[10]} of {scores[10] * 100}%')\n",
    "    loss_per_fold.append(scores[0])\n",
    "    acc_per_fold.append((scores[1] * 100))\n",
    "    auc_per_fold.append((scores[2] * 100))\n",
    "    precision_per_fold.append((scores[3] * 100))\n",
    "    recall_per_fold.append((scores[4] * 100))\n",
    "    prc_per_fold.append(scores[5])\n",
    "    tp_per_fold.append(scores[6])\n",
    "    fp_per_fold.append(scores[7])\n",
    "    tn_per_fold.append(scores[8])\n",
    "    fn_per_fold.append(scores[9])\n",
    "    val_f1scores.append(round((scores[10] * 100), 2))\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Display average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Validation score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {(loss_per_fold[i])} - Accuracy: {acc_per_fold[i]}% - AUC: {auc_per_fold[i]}% - Precision: {precision_per_fold[i]}% - Recall: {recall_per_fold[i]}% - F1-score: {val_f1scores[i]}%')\n",
    "print(\"Execution time for training is: %f\"%(float(end)- float(start)))\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average training scores for all folds:')\n",
    "print(f'> Loss: {np.mean(loss_training_fold)}')\n",
    "print(f'> Accuracy: {(np.mean(acc_training_fold )* 100)} (+- {np.std(acc_training_fold)})')\n",
    "print(f'> AUC: {(np.mean(auc_training_fold)* 100)} (+- {np.std(auc_training_fold)})')\n",
    "print(f'> Precision: {(np.mean(precision_training_fold) * 100)} (+- {np.std(precision_training_fold)})')\n",
    "print(f'> Recall: {(np.mean(recall_training_fold) * 100)} (+- {np.std(recall_training_fold)})')\n",
    "print(f'> F1-score: {(np.mean(training_f1_scores) * 100)} (+- {np.std(training_f1_scores)})')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average validation scores for all folds:')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> AUC: {np.mean(auc_per_fold)} (+- {np.std(auc_per_fold)})')\n",
    "print(f'> Precision: {np.mean(precision_per_fold)} (+- {np.std(precision_per_fold)})')\n",
    "print(f'> Recall: {np.mean(recall_per_fold)} (+- {np.std(recall_per_fold)})')\n",
    "print(f'> F1-score: {np.mean(val_f1scores)} (+- {np.std(val_f1scores)})')\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "# Serialize Model to JSON\n",
    "json_model = model.to_json()\n",
    "with open('dnn_boost_pancreatic_24features.json', 'w') as json_file:\n",
    "    json_file.write(json_model)\n",
    "    \n",
    "# Serialize weights to HDF5\n",
    "model.save_weights('dnn_boost_pancreatic_24features')\n",
    "print(\"Saved model to disk\")\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print('Succesfully trained model!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of the testing data with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the testing data\n",
    "smote_enn=smote_enn.fit(x_test, y_test)\n",
    "x_test_resamp, y_test_resamp = smote_enn.fit_resample(x_test, y_test)\n",
    "y_test_resamp = np.eye(2)[y_test_resamp.flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the prediction of the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_cat = model.predict(x_test_resamp, batch_size=100, verbose=verbosity).round()\n",
    "accuracy_final = accuracy_score(y_test_resamp, y_test_pred_cat)\n",
    "recall_final = recall_score(y_test_resamp, y_test_pred_cat, average='weighted')\n",
    "precision_final = precision_score(y_test_resamp, y_test_pred_cat, average='weighted')\n",
    "#calculate F1 score\n",
    "f1_final = f1_score(y_test_resamp, y_test_pred_cat, average='weighted')\n",
    "print('Accuracy of prediction model:')\n",
    "print(f'> Prediction accuracy: {(accuracy_final * 100)} ')\n",
    "print('Precision of prediction model:')\n",
    "print(f'> Precision: {(precision_final * 100)} ')\n",
    "print('Recall of prediction model:')\n",
    "print(f'> Recall: {(recall_final * 100)} ')\n",
    "print('F1-score of prediction model:')\n",
    "print(f'> F1: {(f1_final * 100)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(test_true_label, predicted_label)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    print('True Somatic Variant Detected (True Positives): ', cm[1][1])\n",
    "    print('Germline Variant Detected (True Negatives): ', cm[0][0])\n",
    "    print('Germline Variant Incorrectly Detected as True Somatic Variant (False Positives): ', cm[0][1])\n",
    "    print('True Somatic Variant Detected Incorrectly Detected as Germline (False Negatives): ', cm[1][0])\n",
    "    print('Total True Somatic Variant: ', np.sum(cm[1]))\n",
    "    print('Total Germline: ', np.sum(cm[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_label = tf.argmax(y_test_resamp, axis = 1)\n",
    "predicted_label = tf.argmax(y_test_pred_cat, axis = 1)\n",
    "plot_cm(test_true_label, predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of the tumor-only dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from the tumor-only_data folder\n",
    "tumor_only_data = pd.read_csv('Pancreatic_cancer_tumor-only.csv',  header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the same 24 features according to the XGBoost feature importance test from the tumor-normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_only_feature24=tumor_only_data[['ExAC_ALL', 'FS', 'MetaLR_score', 'DANN_score', 'gnomAD_exome_ALL',\n",
    "       'MutationTaster_score', 'GenoCanyon_score',\n",
    "       'phastCons100way_vertebrate', 'MetaSVM_score', 'SIFT_score',\n",
    "       'phyloP20way_mammalian', 'SiPhy_29way_logOdds', 'VEST3_score',\n",
    "       'CADD_raw', 'Eigen-raw', 'M-CAP_score', 'MutationAssessor_score',\n",
    "       'phyloP100way_vertebrate', 'QD', 'PROVEAN_score', 'RPB', 'MQ',\n",
    "       'LRT_score', 'CADD_phred','label']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Column Target matching Dictionary value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_labelMut = {'somatic_mutation': 1,\n",
    "                'germline polymorphism': 0}\n",
    "\n",
    "tumor_only_feature24['target'] = tumor_only_feature24['label'].map(map_labelMut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tumor_only_feature24.columns[-1]\n",
    "X = tumor_only_feature24.drop(label, axis = 1)\n",
    "Y = tumor_only_feature24.drop(X, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ohe = X.astype(float).values\n",
    "Y_ohe = Y['target'].values\n",
    "Y_true = np.eye(2)[Y_ohe.flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the tumor-only dataset using the trained model from the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in model from output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('dnn_boost_pancreatic_24features.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model_24 = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights into new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_24.load_weights('dnn_boost_pancreatic_24features')\n",
    "print(\"Loaded model from disk\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the tumor-only dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_24 = loaded_model_24.predict(X_ohe, batch_size=100, verbose=1).round()\n",
    "accuracy_final = accuracy_score(Y_true, y_pred_24)\n",
    "recall_final = recall_score(Y_true, y_pred_24, average='weighted')\n",
    "precision_final = precision_score(Y_true, y_pred_24, average='weighted')\n",
    "#calculate F1 score\n",
    "f1_final = f1_score(Y_true, y_pred_24, average='weighted')\n",
    "print('Accuracy of prediction model:')\n",
    "print(f'> Prediction accuracy: {(accuracy_final * 100)} ')\n",
    "print('Precision of prediction model:')\n",
    "print(f'> Precision: {(precision_final * 100)} ')\n",
    "print('Recall of prediction model:')\n",
    "print(f'> Recall: {(recall_final * 100)} ')\n",
    "print('F1-score of prediction model:')\n",
    "print(f'> F1: {(f1_final * 100)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_label = tf.argmax(Y_true, axis = 1)\n",
    "predicted_label = tf.argmax(y_pred_24, axis = 1)\n",
    "plot_cm(test_true_label, predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we showed the workflow of DNN-Boost, consisted of feature selection with XGBoost and classification with four-layers DNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
